\section{Diagnostics that Matter: From Errors to Explanations}

At the heart of the platform is a weekly diagnostic that is short, focused, and instructionally actionable. Each diagnostic targets a small set of high-leverage skills (for example: comparing fractions with unlike denominators; interpreting points on a number line; solving for an unknown in a one-step equation; reading multiplicative comparisons). The aim is not high-stakes sorting but low-stakes sensing: to illuminate learning in motion.

\subsection{Design Principles for Diagnostic Items}

Diagnostic items in the platform follow three principles:

\begin{enumerate}
  \item \textbf{Elicitable Misconceptions.} Items are constructed so that common misconceptions produce predictable wrong answers. For example, when comparing $\tfrac{1}{4}$ and $\tfrac{1}{8}$, a learner who believes “larger denominator means larger value” will systematically choose $\tfrac{1}{8}$. Such items allow analytics to do more than count errors; they attribute errors to \emph{concepts}.
  \item \textbf{Representation-Rich.} Items are surrounded by supportive representations---fraction bars, number lines, manipulatives, and story contexts---that help learners reason and that give teachers visibility into where understanding breaks down. The system presents these supports not as hints to be toggled away but as part of the mathematical conversation.
  \item \textbf{Short Cycle, High Frequency.} Because the platform is built for weekly cadence, diagnostics are intentionally brief. This allows teachers to receive fresh insight without sacrificing teaching time, and it turns intervention into a rhythm rather than a scramble.
\end{enumerate}

\subsection{From Wrong Answers to Misconception Clusters}

Counting wrong answers is insufficient for instruction. The platform organizes errors into misconception clusters that are interpretable by teachers and aligned to skills. Examples include: ``denominator magnitude bias'' in fraction comparison; ``whole-number bias'' in decimal operations; ``additive overgeneralization'' in proportional reasoning; and ``operator-as-instruction'' in word problems. In the teacher analytics, a \emph{Misconception Dashboard} surfaces the top clusters by class and cohort, with plain-language descriptions and examples drawn from recent responses.

Two properties make these clusters practically useful:

\begin{itemize}
  \item \textbf{Actionability.} Each cluster links to a set of targeted activities or representations. If a class shows a spike in denominator magnitude bias, the platform proposes a short sequence using fraction bars and number lines that build the idea of unit size and partitioning. Teachers can preview the representations and select those that match their learners' needs.
  \item \textbf{Trajectory Awareness.} Clusters are not static taxonomies; they have trend lines. The \emph{Diagnostic Deep Dive} view presents how a misconception's prevalence changes over weeks, after targeted instruction, giving teachers and IR teams a way to see whether instructional responses are working.
\end{itemize}

\subsection{Interpretable Risk, Not Opaque Labels}

Risk analytics in the platform are conveyed as trajectories, not single numbers. The \emph{Predictive Risk Analytics} view shows the share of learners currently on track, in need of support, and at risk, with attention to how quickly support follows identification. Crucially, every risk signal is paired with its instructional rationale: which misconceptions or skills are driving the signal, and what actions are recommended. The point is not to predict failure; it is to prioritize support.

\subsection{Time-to-Intervention as a First-Class Metric}

To make analytics consequential, the system elevates \emph{time-to-intervention} as a core metric. The teacher dashboard displays a distribution of days from first risk signal to first support action, along with the percentage of at-risk learners who received support within seven days. This reorients practice: teachers and advisors do not just watch risk; they shorten the latency from signal to help. Institutional leaders can then measure whether investments---in supplemental instruction, tutoring, or micro-bursaries---translate into shorter latencies and better outcomes.

