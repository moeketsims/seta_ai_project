\section{Evidence and Improvement: An IR Playbook}
\label{sec:evidence}

Institutional Research (IR) is essential for turning promising practices into reliable improvements. The platform therefore ships with an evaluation playbook that fits academic operations. The goal is credible, decision-grade evidence without paralyzing the work of teaching and support.

\subsection{Core Outcomes and Leading Indicators}

The playbook defines a small set of outcomes and leading indicators:
\begin{itemize}
  \item \textbf{Leading indicators} (weekly): risk share, time-to-intervention, mastery on targeted skills, prevalence of key misconception clusters, engagement signals (on-task time, completion of pathway tasks).
  \item \textbf{Intermediate outcomes} (termly): pass rates in gateway modules, credit accumulation rate, re-enrolment (retention) for the next term, movement in equity gaps.
  \item \textbf{Institutional outcomes} (annual): progression, graduation trajectory, directional movement in cost-per-graduate.
\end{itemize}

\subsection{Designs that Respect Operations}

Several quasi-experimental designs are feasible in ordinary academic practice:
\begin{enumerate}
  \item \textbf{Matched Cohorts.} Compare classes or cohorts with similar prior achievement and demographics where the platform is used versus where it is not yet used, while controlling for observable differences.
  \item \textbf{Within-Cohort Phasing.} Introduce certain features (e.g., representation-rich mini-lessons or advisor routing) at different times across sections, creating natural comparisons.
  \item \textbf{Regression Discontinuity at a Policy Threshold.} When small supports (like micro-bursaries for tutoring) are allocated based on a transparent threshold (e.g., sustained risk trajectory), compare learners near the cut-off.
\end{enumerate}

The platformâ€™s logging of actions and timing enables IR teams to align analyses tightly with what actually happened. Disaggregation is expected, not optional, so that equity impacts are visible.

\subsection{Teacher Learning and Organisational Memory}

Evidence is not only about end-states; it is about building staff capability. Teachers can attach short reflections to actions (``the number line helped once we connected to the fraction bar''), and these become searchable snippets linked to metrics. Over time, teaching teams inherit a living repository of what tends to work for particular misconceptions in their context. IR teams can curate periodic briefs that synthesize these reflections with quantitative results.

\subsection{Publishing and Accountability}

Because the platform emphasizes interpretable analytics and ethical data use, it lowers the barrier for transparent reporting to councils, senates, and external quality bodies. A termly ``outcomes and equity'' brief can be generated that shows gains, gaps, and next steps, along with a succinct description of data governance. Accountability becomes a story of capability: what the institution is learning about how to help its students.

